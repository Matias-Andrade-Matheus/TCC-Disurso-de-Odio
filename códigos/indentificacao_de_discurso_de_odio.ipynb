{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t2hZ_vC69_al"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randrange\n",
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lHsWoEtg-IU5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Baixar recursos do NLTK (se necessário)\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = stopwords.words('portuguese')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DVr6Ai7GPzca"
      },
      "outputs": [],
      "source": [
        "stopwords_especificas = [\n",
        "    \"vc\", \"voce\", \"vcs\", \"tá\", \"ta\", \"to\", \"tô\", \"pq\", \"q\", \"né\", \"eh\", \"ai\", \"aí\", \"c\", \"d\", \"já\",\n",
        "    \"pro\", \"pra\", \"pras\", \"nois\", \"tbm\", \"tb\", \"que\", \"oq\", \"ctz\", \"td\", \"toda\", \"todo\", \"todos\", \"ja\",\n",
        "    \"ate\", \"até\", \"dps\", \"qdo\", \"cm\", \"rt\", \"from\", \"rs\", \"aff\", \"dae\", \"dai\", \"so\", \"só\", \"la\", \"lá\",\"cade\",\n",
        "    \"xq\", \"x\", \"tam\", \"vlw\", \"obg\", \"ne\", \"n\", \"ñ\", \"nn\", \"nao\", \"sim\", \"s\", \"ss\", \"pfv\", \"pf\", \"plz\",\n",
        "    \"cadê\", \"kd\", \"aki\", \"aqui\", \"ali\", \"dali\", \"naquele\", \"naquela\", \"naquilo\", \"aonde\", \"dela\", \"dele\",\n",
        "    \"deles\", \"delas\", \"mt\", \"mto\", \"mtos\", \"mta\", \"mtas\", \"oh\", \"ah\", \"ui\", \"ops\", \"lol\", \"ftw\",\n",
        "    \"oxe\", \"argh\", \"eita\", \"ih\", \"ui\", \"epa\", \"oba\", \"ufa\", \"aham\", \"hmm\", \"hm\", \"hi\", \"hein\", \"puf\",\n",
        "    \"tipo\", \"tipo assim\", \"mano\", \"cara\", \"vei\", \"gente\", \"galera\", \"fia\", \"fi\", \"tamo\", \"vamo\", \"bora\",\n",
        "    \"sao\", \"vai\", \"vem\", \"aonde\", \"donde\", \"onde\", \"aqui\", \"dali\", \"ma\", \"meu\", \"minha\", \"nosso\", \"nossa\"\n",
        "]\n",
        "\n",
        "abrevicoes_odiosas = [\n",
        "    \"wtf\", \"pqp\", \"vsfd\", \"sfd\", \"vtmnc\", \"vtmc\", \"tmnk\", \"tmnc\", \"fdp\", \"fds\", \"k7\", \"kct\", \"prr\", \"mrda\", \"merd\", \"mrd\", \"bct\", \"pk\", \"xt\", \"krl\",\n",
        "    \"crl\", \"bct\", \"bp\", \"nc\", \"vdd\", \"viad\", \"vyd\", \"bait\", \"beyt\", \"bixa\", \"bixy\", \"bixa\", \"bixy\", \"fudr\", \"fodr\", \"fodase\", \"crlh\", \"crl\", \"caralh\",\n",
        "    \"carlh\", \"porr\", \"p0rr4\", \"p0rr@\", \"fuder\", \"fud3r\", \"f0d3r\", \"f0der\", \"mcl\", \"m3rd4\", \"m3rd@\", \"merd4\", \"merd@\", \"bocet\", \"b0cet\", \"boc3t\", \"b0c3t\",\n",
        "    \"bucet\", \"buc3t\", \"b0cet\", \"b0c3t\", \"pint0\", \"pint@\", \"p1nt@\", \"p1nt0\", \"rola\", \"r0l@\", \"r0l4\", \"rol4\", \"rol@\", \"vagabund\", \"vagabund@\", \"vagabund4\", \"vagabund0\",\n",
        "    \"vgbd\",\"vadia\", \"vadi4\", \"vadi@\", \"v4di@\", \"v4di4\", \"puta\", \"put4\", \"put@\", \"dbf\", \"aut\", \"rtd\", \"cuz4o\", \"cuz@o\", \"cuz4@\", \"cuz@\", \"stpr\"\n",
        "]\n",
        "    \n",
        "abrevicoes_odiosas = set(abrevicoes_odiosas)\n",
        "\n",
        "stopwords_tradicionais = set(stop_words)\n",
        "\n",
        "stopwords_especificas = set(stopwords_especificas)\n",
        "\n",
        "STOPWORDS_COMPLETA = stopwords_tradicionais.union(stopwords_especificas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "M7drhy0rpDoB"
      },
      "outputs": [],
      "source": [
        "def remover_palavras_exatas(texto: str, palavras: list[str]) -> str:\n",
        "    \"\"\"\n",
        "    Remove palavras exatas do texto usando regex, sem remover substrings.\n",
        "\n",
        "    Args:\n",
        "        texto (str): Texto de entrada.\n",
        "        palavras (list[str]): Lista de palavras a remover (exatamente).\n",
        "\n",
        "    Returns:\n",
        "        str: Texto com as palavras removidas.\n",
        "    \"\"\"\n",
        "    for palavra in palavras:\n",
        "        # Remove a palavra com delimitador de palavra (\\b) e insensível a maiúsculas\n",
        "        padrao = rf'\\b{re.escape(palavra)}\\b'\n",
        "        texto = re.sub(padrao, '', texto, flags=re.IGNORECASE)\n",
        "\n",
        "    # Limpa múltiplos espaços e espaços antes de pontuação\n",
        "    texto = re.sub(r'\\s{2,}', ' ', texto)\n",
        "    texto = re.sub(r'\\s+([,.!?;:])', r'\\1', texto)\n",
        "\n",
        "    return texto.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YVnQ7wyY-V77"
      },
      "outputs": [],
      "source": [
        "# 2. Pré-processamento (corrigido) [99%]\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def clean_text(text, lemmatizer=False):\n",
        "    '''\n",
        "    Perform stop-words removal and lemmatization\n",
        "    '''\n",
        "\n",
        "    text = str(text)\n",
        "\n",
        "    if lemmatizer:\n",
        "        words = [word for word in text.split()]\n",
        "        words = [WordNetLemmatizer().lemmatize(word) for word in words]\n",
        "        return \" \".join(words)\n",
        "    \n",
        "    text = text.lower()\n",
        "\n",
        "    text_normalize = unicodedata.normalize(\"NFKD\", text)\n",
        "    text = ''.join(\n",
        "        char for char in text_normalize\n",
        "        if not unicodedata.combining(char)\n",
        "    )\n",
        "\n",
        "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
        "\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        if re.search('http', word) or re.search('https', word):\n",
        "            text = text.replace(word, '')\n",
        "\n",
        "        if re.search('@', word) or re.search('#', word):\n",
        "            text = text.replace(word, '')\n",
        "\n",
        "        if re.search('kk', word) or re.search(\"hah\", word) or re.search('aha', word):\n",
        "            text = text.replace(word, '')\n",
        "\n",
        "        if word in abrevicoes_odiosas:\n",
        "            for abrev in abrevicoes_odiosas:\n",
        "                if re.search(abrev, word) and len(abrev) == len(word):\n",
        "                    text = text.replace(abrev, 'KKK')\n",
        "                \n",
        "\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    words = [word for word in text.split() if (word not in STOPWORDS_COMPLETA)]\n",
        "\n",
        "\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Criação de Datasets limpos\n",
        "import os\n",
        "import re\n",
        "\n",
        "pasta_atual = os.getcwd()\n",
        "if os.path.exists(pasta_atual+\"/Datasets\"):\n",
        "    pasta_datasets_sujos = os.path.join(pasta_atual, \"Datasets\")\n",
        "    datasets_paths = []\n",
        "\n",
        "    if os.path.exists(pasta_datasets_sujos+\"/OLID - BR\"):\n",
        "        pasta_olid = os.path.join       (pasta_datasets_sujos, \"OLID - BR\")\n",
        "        arquivo_olid = os.path.join(pasta_olid, \"2019-05-28_portuguese_hate_speech_binary_classification.csv\")\n",
        "        datasets_paths.append(arquivo_olid)\n",
        "\n",
        "    if os.path.exists(pasta_datasets_sujos+\"/OffComBR-3\"):\n",
        "        pasta_offcomTres = os.path.join(pasta_datasets_sujos, \"OffComBR-3\")\n",
        "        arquivo_offcomTres = os.path.join(pasta_offcomTres, \"OffComBR3.csv\")\n",
        "        datasets_paths.append(arquivo_offcomTres)\n",
        "\n",
        "    if os.path.exists(pasta_datasets_sujos+\"/Offcom2\"):\n",
        "        pasta_offcomDois = os.path.join   (pasta_datasets_sujos, \"Offcom2\")\n",
        "        arquivo_offcomDois = os.path.join(pasta_offcomDois, \"OffComBR2.csv\")\n",
        "        datasets_paths.append(arquivo_offcomDois)\n",
        "\n",
        "    if os.path.exists(pasta_datasets_sujos+\"/ToLD\"):\n",
        "        pasta_told = os.path.join            (pasta_datasets_sujos, \"ToLD\")\n",
        "        arquivo_told = os.path.join(pasta_told, \"ToLD-BR_binario.csv\")\n",
        "        datasets_paths.append(arquivo_told)\n",
        "\n",
        "    if os.path.exists(pasta_datasets_sujos+\"/hateBR\"):\n",
        "        pasta_hate = os.path.join          (pasta_datasets_sujos, \"hateBR\")\n",
        "        arquivo_hate = os.path.join(pasta_hate, \"HateBR.csv\")\n",
        "        datasets_paths.append(arquivo_hate)\n",
        "    \n",
        "    # multioffcom3\n",
        "    if os.path.exists(pasta_datasets_sujos+\"/OffComBR-3\"):\n",
        "        pasta_multiOffcomTres = os.path.join(pasta_datasets_sujos, \"OffComBR-3\")\n",
        "        arquivo_multiOffcomTres = os.path.join(pasta_multiOffcomTres, \"MultiOffComBR3.csv\")\n",
        "        datasets_paths.append(arquivo_multiOffcomTres)\n",
        "\n",
        "text=['text','mensagem','comentario','text','comentario']\n",
        "label = ['hatespeech_comb','label','label','Discurso_de_odio','label_final']\n",
        "\n",
        "if not os.path.exists(\"Datasets Limpos\"):\n",
        "    os.mkdir(\"Datasets Limpos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8p4A3QSFo0qb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coleção de dados : 2019-05-28_portuguese_hate_speech_binary_classification.csv\n",
            "Shape : (5670, 8)\n",
            "Instâncias : hatespeech_comb\n",
            "0    3882\n",
            "1    1788\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "Coleção de dados : OffComBR3.csv\n",
            "Shape : (1033, 2)\n",
            "Instâncias : label\n",
            "no     831\n",
            "yes    202\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "Coleção de dados : OffComBR2.csv\n",
            "Shape : (1250, 2)\n",
            "Instâncias : label\n",
            "no     831\n",
            "yes    419\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "Coleção de dados : ToLD-BR_binario.csv\n",
            "Shape : (21000, 15)\n",
            "Instâncias : Discurso_de_odio\n",
            "0    11742\n",
            "1     9258\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "Coleção de dados : HateBR.csv\n",
            "Shape : (7000, 8)\n",
            "Instâncias : label_final\n",
            "1    3500\n",
            "0    3500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "datasets = [\n",
        "    'Datasets/OLID - BR/2019-05-28_portuguese_hate_speech_binary_classification.csv',\n",
        "    'Datasets/OffComBR-3/OffComBR3.csv',\n",
        "    'Datasets/Offcom2/OffComBR2.csv',\n",
        "    'Datasets/ToLD/ToLD-BR_binario.csv',\n",
        "    'Datasets/hateBR/HateBR.csv'\n",
        "]\n",
        "\n",
        "text=['text','mensagem','comentario','text','comentario']\n",
        "label = ['hatespeech_comb','label','label','Discurso_de_odio','label_final']\n",
        "for dataset,label in zip(datasets,label) :\n",
        "  df = pd.read_csv(dataset)\n",
        "  print(\"Coleção de dados : {}\\nShape : {}\\nInstâncias : {}\\n\\n\".format(\n",
        "      dataset.split('/')[-1],\n",
        "      df.shape,\n",
        "      df[label].value_counts()\n",
        "      ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "kmKEzQIV4rZ6",
        "outputId": "54241476-e4ed-4a82-ee14-1321371404e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto Original : RT @pauloap: Porra, olha a série de reportagens que o Jornal da Record estreia hoje, nunca que a Globo faria https://t.co/1QzI4oabRD\n",
            "Texto Limpo : porra olha serie reportagens jornal record estreia hoje nunca globo faria\n",
            "Texto Limpo e Lemmatizado : RT @pauloap: Porra, olha a série de reportagens que o Jornal da Record estreia hoje, nunca que a Globo faria https://t.co/1QzI4oabRD\n",
            "\n",
            "\n",
            "Texto Original : 'O que esta OPOSICAO esta conseguindo e atrair para si e para os proximos e o odio dos eleitores '\n",
            "Texto Limpo : oposicao conseguindo atrair si proximos odio eleitores\n",
            "Texto Limpo e Lemmatizado : 'O que esta OPOSICAO esta conseguindo e atrair para si e para o proximos e o odio do eleitores '\n",
            "\n",
            "\n",
            "Texto Original : 'Um volante da base PELO AMOR DE DEUS para o lugar de Marcio AraujoPELA AMOR'\n",
            "Texto Limpo : volante base amor deus lugar marcio araujopela amor\n",
            "Texto Limpo e Lemmatizado : 'Um volante da base PELO AMOR DE DEUS para o lugar de Marcio AraujoPELA AMOR'\n",
            "\n",
            "\n",
            "Texto Original : rt @user vejo a ana e a chai mais do que a minha família\n",
            "Texto Limpo : vejo ana chai familia\n",
            "Texto Limpo e Lemmatizado : rt @user vejo a ana e a chai mais do que a minha família\n",
            "\n",
            "\n",
            "Texto Original : Mostro sempre Benedita falando para minhas filhas branquinhas... quero beneditas ao meu redor!!!! Fora racistas.!!!!\n",
            "Texto Limpo : mostro sempre benedita falando filhas branquinhas quero beneditas redor racistas\n",
            "Texto Limpo e Lemmatizado : Mostro sempre Benedita falando para minhas filhas branquinhas... quero beneditas ao meu redor!!!! Fora racistas.!!!!\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "for text_,dataset in zip(text,datasets):\n",
        "  df = pd.read_csv(dataset)\n",
        "  texto = df[text_]\n",
        "  texto = texto[random.randrange(len(df))]\n",
        "  limpo = clean_text(texto)\n",
        "  lemmatizado = clean_text(texto, lemmatizer=True)\n",
        "  print(\"Texto Original : {}\\nTexto Limpo : {}\\nTexto Limpo e Lemmatizado : {}\\n\\n\".format(texto,limpo,lemmatizado))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      Dataset  # Instâncias  Tamanho Médio (palavras)  \\\n",
            "0   OLID - BR          5670                     15.81   \n",
            "1  OffComBR-3          1033                     13.70   \n",
            "2     Offcom2          1250                     13.21   \n",
            "3        ToLD         21000                     15.62   \n",
            "4      hateBR          7000                     13.93   \n",
            "\n",
            "   Tamanho Médio (caracteres)  # Classes Distribuição de Classes  \n",
            "0                      101.35          2        0: 3882, 1: 1788  \n",
            "1                       77.21          2       no: 831, yes: 202  \n",
            "2                       74.56          2       no: 831, yes: 419  \n",
            "3                       87.02          2       0: 11742, 1: 9258  \n",
            "4                       81.96          2        1: 3500, 0: 3500  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Lista de datasets e suas respectivas colunas\n",
        "datasets = [\n",
        "    'Datasets/OLID - BR/2019-05-28_portuguese_hate_speech_binary_classification.csv',\n",
        "    'Datasets/OffComBR-3/OffComBR3.csv',\n",
        "    'Datasets/Offcom2/OffComBR2.csv',\n",
        "    'Datasets/ToLD/ToLD-BR_binario.csv',\n",
        "    'Datasets/hateBR/HateBR.csv'\n",
        "]\n",
        "\n",
        "text_columns = ['text', 'mensagem', 'comentario', 'text', 'comentario']\n",
        "label_columns = ['hatespeech_comb', 'label', 'label', 'Discurso_de_odio', 'label_final']\n",
        "\n",
        "results = []\n",
        "try:\n",
        "    # Ler o dataset para criação Global\n",
        "    df = pd.read_csv(datasets[0])\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao processar {dataset[0]}: {str(e)}\")\n",
        "\n",
        "for i, dataset_path in enumerate(datasets):\n",
        "    try:\n",
        "        # Ler o dataset\n",
        "        df = pd.read_csv(dataset_path)\n",
        "        \n",
        "        # Obter colunas de texto e label\n",
        "        text_col = text_columns[i]\n",
        "        label_col = label_columns[i]\n",
        "        \n",
        "        # Calcular estatísticas\n",
        "        num_instances = len(df)\n",
        "        \n",
        "        # Tamanho médio em palavras\n",
        "        avg_word_length = df[text_col].apply(lambda x: len(str(x).split())).mean()\n",
        "        \n",
        "        # Tamanho médio em caracteres\n",
        "        avg_char_length = df[text_col].apply(lambda x: len(str(x))).mean()\n",
        "        \n",
        "        # Número de classes\n",
        "        num_classes = df[label_col].nunique()\n",
        "        \n",
        "        # Distribuição de classes\n",
        "        class_distribution = df[label_col].value_counts().to_dict()\n",
        "        \n",
        "        # Formatar distribuição para melhor visualização\n",
        "        dist_str = \", \".join([f\"{k}: {v}\" for k, v in class_distribution.items()])\n",
        "        \n",
        "        results.append({\n",
        "            'Dataset': dataset_path.split('/')[1],\n",
        "            '# Instâncias': num_instances,\n",
        "            'Tamanho Médio (palavras)': round(avg_word_length, 2),\n",
        "            'Tamanho Médio (caracteres)': round(avg_char_length, 2),\n",
        "            '# Classes': num_classes,\n",
        "            'Distribuição de Classes': dist_str\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao processar {dataset_path}: {str(e)}\")\n",
        "\n",
        "# Criar DataFrame com resultados\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n",
        "\n",
        "# Salvar resultados em CSV\n",
        "# results_df.to_csv('estatisticas_datasets.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KRfxKpQWFcd_"
      },
      "outputs": [],
      "source": [
        "# 2.1. Data Frame Treatment & Training and Matching Separation\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_and_prepare_data(filepath, text_colum, label_colum):\n",
        "    \"\"\"Carrega e prepara os dados\"\"\"\n",
        "    # Carregar dados\n",
        "    df_TextLabel = pd.read_csv(filepath)\n",
        "\n",
        "    # Limpar textos\n",
        "    df_TextLabel['cleaned_text'] = df_TextLabel[text_colum].apply(clean_text)\n",
        "\n",
        "    # Filtrar colunas necessárias\n",
        "    df_TextLabel = df_TextLabel[['cleaned_text', label_colum]]\n",
        "    df_TextLabel.columns = ['text', 'label']\n",
        "\n",
        "    # Remover linhas vazias\n",
        "    df_TextLabel = df_TextLabel.dropna()\n",
        "    df_TextLabel = df_TextLabel[df_TextLabel['text'] != '']\n",
        "\n",
        "    return df_TextLabel\n",
        "\n",
        "def split_data(df_TextLabel):\n",
        "    \"\"\"Divide os dados em treino e teste\"\"\"\n",
        "    text_var = df_TextLabel['text']\n",
        "    label_var = df_TextLabel['label']\n",
        "    return train_test_split(text_var, label_var, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4BQCCoFGFpS"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'label'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mKeyError\u001b[39m: 'label'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m classes = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.unique()\n\u001b[32m      7\u001b[39m random_class = classes[randrange(\u001b[38;5;28mlen\u001b[39m(classes))]\n\u001b[32m      9\u001b[39m class_text = \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(df[df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m] == random_class][\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
            "\u001b[31mKeyError\u001b[39m: 'label'"
          ]
        }
      ],
      "source": [
        "# 2.2.1. Data Exploration & Visualization (Word Cloud)\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('Datasets/OLID - BR/2019-05-28_portuguese_hate_speech_binary_classification.csv')\n",
        "classes = df['label'].unique()\n",
        "random_class = classes[randrange(len(classes))]\n",
        "\n",
        "class_text = ' '.join(df[df['label'] == random_class]['text'])\n",
        "class_text = ' '.join(df['text'])\n",
        "\n",
        "# Gerar a nuvem de palavras\n",
        "wordcloud = WordCloud(width=800, height=800,\n",
        "                         background_color='black',\n",
        "                         stopwords=\"portuguese\",\n",
        "                         collocations=True).generate(class_text)\n",
        "\n",
        "# Plotar\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(f'Nuvem de Palavras para Classe: {random_class}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "aOMzKN7RTQqQ",
        "outputId": "6b8c1e1b-f1b3-4cd5-befa-7d65c831d4bc"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'label'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m plt.figure(figsize=(\u001b[32m6\u001b[39m, \u001b[32m4\u001b[39m))\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Contando os textos por categoria e plotando\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory_column\u001b[49m\u001b[43m)\u001b[49m.size().sort_values(ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).plot.bar(color=\u001b[33m'\u001b[39m\u001b[33m#1f77b4\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Adicionando título e labels\u001b[39;00m\n\u001b[32m     12\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mDistribuição de Textos por Categoria no HateBR\u001b[39m\u001b[33m\"\u001b[39m, pad=\u001b[32m20\u001b[39m, fontsize=\u001b[32m14\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\frame.py:9190\u001b[39m, in \u001b[36mDataFrame.groupby\u001b[39m\u001b[34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   9187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   9188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to supply one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m9190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   9192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9193\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9196\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9200\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\groupby\\groupby.py:1330\u001b[39m, in \u001b[36mGroupBy.__init__\u001b[39m\u001b[34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   1327\u001b[39m \u001b[38;5;28mself\u001b[39m.dropna = dropna\n\u001b[32m   1329\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1330\u001b[39m     grouper, exclusions, obj = \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m   1341\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping._passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper.groupings):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[39m, in \u001b[36mget_grouper\u001b[39m\u001b[34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[39m\n\u001b[32m   1041\u001b[39m         in_axis, level, gpr = \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1042\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr.key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1045\u001b[39m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[32m   1046\u001b[39m     exclusions.add(gpr.key)\n",
            "\u001b[31mKeyError\u001b[39m: 'label'"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 2.2.2. Visualisation (Visualizing word count balance in bar chart) [99%]\n",
        "\n",
        "category_column = \"label\"\n",
        "\n",
        "# Criando o gráfico\n",
        "plt.figure(figsize=(6, 4))\n",
        "\n",
        "# Contando os textos por categoria e plotando\n",
        "df.groupby(category_column).size().sort_values(ascending=False).plot.bar(color='#1f77b4')\n",
        "\n",
        "# Adicionando título e labels\n",
        "plt.title(\"Distribuição de Textos por Categoria no HateBR\", pad=20, fontsize=14)\n",
        "plt.xlabel(\"Categoria\", labelpad=10)\n",
        "plt.ylabel(\"Número de Textos\", labelpad=10)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# Ajustando layout para não cortar rótulos\n",
        "plt.tight_layout()\n",
        "\n",
        "# Mostrando o gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "I6pO2Or83pfa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('Datasets/OffComBR-3/Multiclasse - OffComBR3.xlsx')\n",
        "multioffcom = df.copy(True).to_csv('Datasets/OffComBR-3/MultiOffComBR3.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando processo de treinamento e avaliação...\n",
            "\n",
            "==================================================\n",
            "Processando dataset: HateBR\n",
            "==================================================\n",
            "Vectorização concluída\n",
            "\n",
            "Treinando MultinomialNB...\n",
            "  Acurácia: 0.8233\n",
            "  F1-Score: 0.8233\n",
            "\n",
            "Treinando RandomForestClassifier...\n",
            "  Acurácia: 0.8076\n",
            "  F1-Score: 0.8071\n",
            "\n",
            "Treinando LinearSVC...\n",
            "  Acurácia: 0.8190\n",
            "  F1-Score: 0.8188\n",
            "\n",
            "Treinando LogisticRegression...\n",
            "  Acurácia: 0.8069\n",
            "  F1-Score: 0.8066\n",
            "\n",
            "Treinando KNeighborsClassifier...\n",
            "  Acurácia: 0.6860\n",
            "  F1-Score: 0.6855\n",
            "\n",
            "==================================================\n",
            "Processando dataset: Offcom2\n",
            "==================================================\n",
            "Vectorização concluída\n",
            "\n",
            "Treinando MultinomialNB...\n",
            "  Acurácia: 0.6988\n",
            "  F1-Score: 0.6740\n",
            "\n",
            "Treinando RandomForestClassifier...\n",
            "  Acurácia: 0.6345\n",
            "  F1-Score: 0.6477\n",
            "\n",
            "Treinando LinearSVC...\n",
            "  Acurácia: 0.6827\n",
            "  F1-Score: 0.6741\n",
            "\n",
            "Treinando LogisticRegression...\n",
            "  Acurácia: 0.6908\n",
            "  F1-Score: 0.6578\n",
            "\n",
            "Treinando KNeighborsClassifier...\n",
            "  Acurácia: 0.5301\n",
            "  F1-Score: 0.5413\n",
            "\n",
            "==================================================\n",
            "Processando dataset: OffcomBR-3\n",
            "==================================================\n",
            "Vectorização concluída\n",
            "\n",
            "Treinando MultinomialNB...\n",
            "  Acurácia: 0.8204\n",
            "  F1-Score: 0.7522\n",
            "\n",
            "Treinando RandomForestClassifier...\n",
            "  Acurácia: 0.7864\n",
            "  F1-Score: 0.7628\n",
            "\n",
            "Treinando LinearSVC...\n",
            "  Acurácia: 0.8010\n",
            "  F1-Score: 0.7591\n",
            "\n",
            "Treinando LogisticRegression...\n",
            "  Acurácia: 0.8155\n",
            "  F1-Score: 0.7327\n",
            "\n",
            "Treinando KNeighborsClassifier...\n",
            "  Acurácia: 0.7961\n",
            "  F1-Score: 0.7445\n",
            "\n",
            "==================================================\n",
            "Processando dataset: OLID-BR\n",
            "==================================================\n",
            "Vectorização concluída\n",
            "\n",
            "Treinando MultinomialNB...\n",
            "  Acurácia: 0.7432\n",
            "  F1-Score: 0.7083\n",
            "\n",
            "Treinando RandomForestClassifier...\n",
            "  Acurácia: 0.7476\n",
            "  F1-Score: 0.7347\n",
            "\n",
            "Treinando LinearSVC...\n",
            "  Acurácia: 0.7449\n",
            "  F1-Score: 0.7334\n",
            "\n",
            "Treinando LogisticRegression...\n",
            "  Acurácia: 0.7502\n",
            "  F1-Score: 0.7185\n",
            "\n",
            "Treinando KNeighborsClassifier...\n",
            "  Acurácia: 0.7193\n",
            "  F1-Score: 0.6643\n",
            "\n",
            "==================================================\n",
            "Processando dataset: BiToLD\n",
            "==================================================\n",
            "Vectorização concluída\n",
            "\n",
            "Treinando MultinomialNB...\n",
            "  Acurácia: 0.7188\n",
            "  F1-Score: 0.7136\n",
            "\n",
            "Treinando RandomForestClassifier...\n",
            "  Acurácia: 0.7541\n",
            "  F1-Score: 0.7548\n",
            "\n",
            "Treinando LinearSVC...\n",
            "  Acurácia: 0.7192\n",
            "  F1-Score: 0.7177\n",
            "\n",
            "Treinando LogisticRegression...\n",
            "  Acurácia: 0.7397\n",
            "  F1-Score: 0.7358\n",
            "\n",
            "Treinando KNeighborsClassifier...\n",
            "  Acurácia: 0.6245\n",
            "  F1-Score: 0.5856\n",
            "\n",
            "==================================================\n",
            "Processando dataset: MultiOffcomBR-3\n",
            "==================================================\n",
            "Erro ao processar dataset MultiOffcomBR-3: \"['label_final'] not in index\"\n",
            "\n",
            "\n",
            "============================================================\n",
            "RESUMO FINAL DOS RESULTADOS\n",
            "============================================================\n",
            "\n",
            "Dataset: HateBR\n",
            "----------------------------------------\n",
            "MultinomialNB             | Acurácia: 0.8233 | F1: 0.8233\n",
            "RandomForestClassifier    | Acurácia: 0.8076 | F1: 0.8071\n",
            "LinearSVC                 | Acurácia: 0.8190 | F1: 0.8188\n",
            "LogisticRegression        | Acurácia: 0.8069 | F1: 0.8066\n",
            "KNeighborsClassifier      | Acurácia: 0.6860 | F1: 0.6855\n",
            "\n",
            "Dataset: Offcom2\n",
            "----------------------------------------\n",
            "MultinomialNB             | Acurácia: 0.6988 | F1: 0.6740\n",
            "RandomForestClassifier    | Acurácia: 0.6345 | F1: 0.6477\n",
            "LinearSVC                 | Acurácia: 0.6827 | F1: 0.6741\n",
            "LogisticRegression        | Acurácia: 0.6908 | F1: 0.6578\n",
            "KNeighborsClassifier      | Acurácia: 0.5301 | F1: 0.5413\n",
            "\n",
            "Dataset: OffcomBR-3\n",
            "----------------------------------------\n",
            "MultinomialNB             | Acurácia: 0.8204 | F1: 0.7522\n",
            "RandomForestClassifier    | Acurácia: 0.7864 | F1: 0.7628\n",
            "LinearSVC                 | Acurácia: 0.8010 | F1: 0.7591\n",
            "LogisticRegression        | Acurácia: 0.8155 | F1: 0.7327\n",
            "KNeighborsClassifier      | Acurácia: 0.7961 | F1: 0.7445\n",
            "\n",
            "Dataset: OLID-BR\n",
            "----------------------------------------\n",
            "MultinomialNB             | Acurácia: 0.7432 | F1: 0.7083\n",
            "RandomForestClassifier    | Acurácia: 0.7476 | F1: 0.7347\n",
            "LinearSVC                 | Acurácia: 0.7449 | F1: 0.7334\n",
            "LogisticRegression        | Acurácia: 0.7502 | F1: 0.7185\n",
            "KNeighborsClassifier      | Acurácia: 0.7193 | F1: 0.6643\n",
            "\n",
            "Dataset: BiToLD\n",
            "----------------------------------------\n",
            "MultinomialNB             | Acurácia: 0.7188 | F1: 0.7136\n",
            "RandomForestClassifier    | Acurácia: 0.7541 | F1: 0.7548\n",
            "LinearSVC                 | Acurácia: 0.7192 | F1: 0.7177\n",
            "LogisticRegression        | Acurácia: 0.7397 | F1: 0.7358\n",
            "KNeighborsClassifier      | Acurácia: 0.6245 | F1: 0.5856\n",
            "\n",
            "Resultados salvos em 'resultados_classificacao.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import time\n",
        "\n",
        "# Download dos recursos do NLTK se necessário\n",
        "try:\n",
        "    from nltk.corpus import stopwords\n",
        "except ImportError:\n",
        "    import nltk\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "def train_and_evaluate_classifiers(datasets_configs, classifiers):\n",
        "    \"\"\"\n",
        "    Treina e avalia múltiplos classificadores em múltiplos datasets\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    for dataset_config in datasets_configs:\n",
        "        dataset_name = dataset_config.get(\"name\", \"unknown_dataset\")\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Processando dataset: {dataset_name}\")\n",
        "        print(f\"{'='*50}\")\n",
        "        \n",
        "        try:\n",
        "            # Carrega e prepara os dados\n",
        "            df = load_and_prepare_data(\n",
        "                dataset_config[\"filepath\"],\n",
        "                dataset_config[\"text_column\"],\n",
        "                dataset_config[\"label_column\"]\n",
        "            )\n",
        "            \n",
        "            # Divide os dados\n",
        "            text_train, text_test, label_train, label_test = split_data(df)\n",
        "            \n",
        "            # Vectorização TF-IDF (apenas uma vez por dataset)\n",
        "            tfidf_vectorizer = TfidfVectorizer(\n",
        "                min_df=5, \n",
        "                ngram_range=(1, 2),\n",
        "                max_features=10000\n",
        "            )\n",
        "            X_train = tfidf_vectorizer.fit_transform(text_train)\n",
        "            X_test = tfidf_vectorizer.transform(text_test)\n",
        "            print(f\"Vectorização concluída\")\n",
        "            \n",
        "            # Treina e avalia cada classificador\n",
        "            for classifier in classifiers:\n",
        "                classifier_name = classifier.__class__.__name__\n",
        "                print(f\"\\nTreinando {classifier_name}...\")\n",
        "                \n",
        "                # Treina o classificador\n",
        "                classifier.fit(X_train, label_train)\n",
        "                \n",
        "                # Faz predições\n",
        "                predictions = classifier.predict(X_test)\n",
        "                \n",
        "                # Calcula métricas\n",
        "                accuracy = accuracy_score(label_test, predictions)\n",
        "                f1 = f1_score(label_test, predictions, average='weighted')\n",
        "                \n",
        "                # Armazena resultados\n",
        "                if dataset_name not in results:\n",
        "                    results[dataset_name] = {}\n",
        "                \n",
        "                results[dataset_name][classifier_name] = {\n",
        "                    'accuracy': accuracy,\n",
        "                    'f1_score': f1,\n",
        "                    'model': classifier,\n",
        "                    'vectorizer': tfidf_vectorizer,\n",
        "                    'predictions': predictions,\n",
        "                    'true_labels': label_test.values\n",
        "                }\n",
        "                \n",
        "                print(f\"  Acurácia: {accuracy:.4f}\")\n",
        "                print(f\"  F1-Score: {f1:.4f}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao processar dataset {dataset_name}: {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Configurações dos datasets\n",
        "datasets = [\n",
        "    {  # HateBR\n",
        "        \"name\": \"HateBR\",\n",
        "        \"filepath\": datasets_paths[4],\n",
        "        \"text_column\": 'comentario',\n",
        "        \"label_column\": 'label_final'\n",
        "    },\n",
        "    {  # Offcom2\n",
        "        \"name\": \"Offcom2\",\n",
        "        \"filepath\": datasets_paths[2],\n",
        "        \"text_column\": 'comentario',\n",
        "        \"label_column\": 'label'\n",
        "    },\n",
        "    {  # OffcomBR-3\n",
        "        \"name\": \"OffcomBR-3\",\n",
        "        \"filepath\": datasets_paths[1],\n",
        "        \"text_column\": 'mensagem',\n",
        "        \"label_column\": 'label'\n",
        "    },\n",
        "    {  # OLID-BR \n",
        "        \"name\": \"OLID-BR\",\n",
        "        \"filepath\": datasets_paths[0],\n",
        "        \"text_column\": 'text',\n",
        "        \"label_column\": 'hatespeech_comb'\n",
        "    },\n",
        "    {  # BiToLD\n",
        "        \"name\": \"BiToLD\",\n",
        "        \"filepath\": datasets_paths[3],\n",
        "        \"text_column\": 'text',\n",
        "        \"label_column\": 'Discurso_de_odio'\n",
        "    },\n",
        "    {   # MultiOffcomBR-3\n",
        "        \"name\": \"MultiOffcomBR-3\",\n",
        "        \"filepath\": datasets_paths[5],\n",
        "        \"text_column\": 'mensagem',\n",
        "        \"label_column\": 'label_final'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Classificadores (instanciados com parâmetros otimizados)\n",
        "classifiers = [\n",
        "    MultinomialNB(),\n",
        "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    LinearSVC(random_state=42, max_iter=1000),\n",
        "    LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1),\n",
        "    KNeighborsClassifier(n_jobs=-1)\n",
        "]\n",
        "\n",
        "# Executa treinamento e avaliação\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Iniciando processo de treinamento e avaliação...\")\n",
        "    results = train_and_evaluate_classifiers(datasets, classifiers)\n",
        "    \n",
        "    # Exibe resultados resumidos\n",
        "    print(\"\\n\\n\" + \"=\"*60)\n",
        "    print(\"RESUMO FINAL DOS RESULTADOS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for dataset_name, dataset_results in results.items():\n",
        "        print(f\"\\nDataset: {dataset_name}\")\n",
        "        print(\"-\" * 40)\n",
        "        for classifier_name, metrics in dataset_results.items():\n",
        "            print(f\"{classifier_name:25} | Acurácia: {metrics['accuracy']:.4f} | F1: {metrics['f1_score']:.4f}\")\n",
        "    \n",
        "    # Salva resultados em arquivo CSV\n",
        "    results_df = pd.DataFrame()\n",
        "    for dataset_name, dataset_results in results.items():\n",
        "        for classifier_name, metrics in dataset_results.items():\n",
        "            results_df = pd.concat([results_df, pd.DataFrame({\n",
        "                'Dataset': [dataset_name],\n",
        "                'Classificador': [classifier_name],\n",
        "                'Acurácia': [metrics['accuracy']],\n",
        "                'F1-Score': [metrics['f1_score']],\n",
        "            })], ignore_index=True)\n",
        "    \n",
        "    results_df.to_csv('resultados_classificacao.csv', index=False)\n",
        "    print(\"\\nResultados salvos em 'resultados_classificacao.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KxuIyS-tVduo"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 5. Analysis (Matriz)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for dataset_config in datasets:\n",
        "    df = load_and_prepare_data(dataset_config[\"filepath\"],\n",
        "                               dataset_config[\"text_column\"],\n",
        "                               dataset_config[\"label_column\"])\n",
        "    \n",
        "    text_train, text_test, label_train, label_test = split_data(df)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(min_df=5, ngram_range=(1, 2), max_features=10000)\n",
        "    X_train = vectorizer.fit_transform(text_train)\n",
        "    X_test = vectorizer.transform(text_test)\n",
        "\n",
        "    for classifier in classifiers:\n",
        "        classifier.fit(X_train, label_train)\n",
        "        y_pred = classifier.predict(X_test)\n",
        "        predictions.append({\n",
        "            \"dataset\": dataset_config[\"name\"],\n",
        "            \"classifier\": classifier.__class__.__name__,\n",
        "            \"label_test\": label_test,\n",
        "            \"y_pred\": y_pred\n",
        "            })\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    present_prediction = predictions[0]  # Exemplo: pegar a primeira predição para visualização\n",
        "    dataset_name = present_prediction[\"dataset\"]\n",
        "    classifier_name = present_prediction[\"classifier\"]\n",
        "\n",
        "    label_test = present_prediction[\"label_test\"]\n",
        "    y_pred = present_prediction[\"y_pred\"]\n",
        "    confusion_metrics = confusion_matrix(label_test, y_pred)\n",
        "\n",
        "    class_names = np.unique(np.concatenate((label_test, y_pred)))\n",
        "    sns.heatmap(confusion_metrics, annot=True, fmt='d', cmap='Greens', xticklabels=class_names, yticklabels=class_names)\n",
        "\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('Actual Labels')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "s8WI9I-qVDlx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 4.2 Classification (Exemple Prediction) [0% ainda não testei nem estudei]\n",
        "teste = tfidf_vectorizer.transform([\"Você é um lindo\"])\n",
        "rf.predict(teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "iSellO1Klg54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 4.2 Classification (Exemple Prediction) [0% ainda não testei nem estudei]\n",
        "teste = tfidf_vectorizer.transform([\"Você é um lixo\"])\n",
        "rf.predict(teste)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
